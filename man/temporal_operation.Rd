% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wrappers.R
\name{temporal_operation}
\alias{temporal_operation}
\title{Perform Temporal Summary Operations Within `NEMO-MEDUSA` by Model Output File Type}
\usage{
temporal_operation(data, analysis = NULL, time_op = NULL, ...)
}
\arguments{
\item{data}{A dataframe containing the metadata of multiple netcdf files.}

\item{analysis}{A text string dictating the type of summary to create. Deprecated, supports "StrathE2E", "1D", & "slabR" for back compatibility.}

\item{time_op}{A text string dictating the type of summary to create. Currently supported summaries include "mean" & "collect". When using time_op all spatial summaries are controlled by slabR schemes.}

\item{...}{Additional arguments passed to the relevant `get_*` function.}
}
\value{
The function returns a dataframe if concatenating, or a matrix if averaging, containing the spatial summaries
of NEMO-MEDUSA model output.
}
\description{
This function takes the metadata for multiple netcdf files, passes it to be summarised spatially according to 
`analysis`, and then performs a temporal operation, either binding the time steps together, or averaging them.
}
\details{
The function bridges the step between extracting data from a netcdf file, and creating a summary of all variables
for a chunk of NEMO-MEDUSA model outputs.

Different file types require a different get function. This function takes a collection of netcdf files of
the same time from the same month, and passes them to the correct `get_*` for data extraction. The results
are passed back to this function for handling the time dimension.

Depending on the analysis chosen, operations performed are: 

- 1D, no further steps.
- StrathE2E or slabR, estimates from different days at the same lat-lon-depth combination are averaged to get a single 
number for the target time step.

Creating intermediate summary objects allows the terabytes of data to be reduced without encountering memory issues.
Also, working on independent packets of data means we can parallelise any data processing for speed.
}
